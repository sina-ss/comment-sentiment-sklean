{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading Data Without Preprocessing\n",
    "\n",
    "This is the simplest case where we just load the data without any modification to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment sentiment\n",
      "0  Oh my god, it just doesn't get any worse than ...  negative\n",
      "1  If you're a layman interested in quantum theor...  negative\n",
      "2  It's amazing that this no talent actor Chapa g...  negative\n",
      "3  This must be one of the most overrated Spanish...  negative\n",
      "4  Some critics have compared Chop Shop with the ...  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_data_without_preprocessing(file_path):\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Usage\n",
    "file_path = 'dataset.csv'\n",
    "data = read_data_without_preprocessing(file_path)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic Text Preprocessing\n",
    "\n",
    "This method includes converting all letters to lower case, removing numbers, extra characters, and separating words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment sentiment\n",
      "0  oh my god it just doesn t get any worse than t...  negative\n",
      "1  if you re a layman interested in quantum theor...  negative\n",
      "2  it s amazing that this no talent actor chapa g...  negative\n",
      "3  this must be one of the most overrated spanish...  negative\n",
      "4  some critics have compared chop shop with the ...  positive\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_text_preprocessing(data):\n",
    "    # Convert to lower case\n",
    "    data['comment'] = data['comment'].str.lower()\n",
    "    # Remove numbers and extra characters\n",
    "    data['comment'] = data['comment'].apply(lambda x: re.sub(r'\\d+', '', x))  # Remove digits\n",
    "    data['comment'] = data['comment'].apply(lambda x: re.sub(r'\\W+', ' ', x))  # Remove non-word characters\n",
    "    return data\n",
    "\n",
    "# Apply basic preprocessing\n",
    "data_basic_preprocessed = basic_text_preprocessing(data.copy())\n",
    "print(data_basic_preprocessed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. High-level Text Preprocessing\n",
    "\n",
    "This method includes all steps from the basic preprocessing plus removal of stopwords, stemming, and lemmatization. Additional methods from online references for text data preprocessing can be explored, such as using `nltk` for stopwords, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment sentiment\n",
      "0  oh god get wors alway love silli littl sci fi ...  negative\n",
      "1  layman interest quantum theori string theori r...  negative\n",
      "2  amaz talent actor chapa got well known star ap...  negative\n",
      "3  must one overr spanish film histori lack subtl...  negative\n",
      "4  critic compar chop shop theatric releas citi g...  positive\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def high_level_text_preprocessing(data):\n",
    "    # Initialize NLTK tools\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Remove stopwords, then stem and lemmatize the words\n",
    "    def preprocess_text(text):\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "    \n",
    "    data['comment'] = data['comment'].apply(preprocess_text)\n",
    "    return data\n",
    "\n",
    "# Apply high-level preprocessing\n",
    "data_high_level_preprocessed = high_level_text_preprocessing(data_basic_preprocessed.copy())\n",
    "print(data_high_level_preprocessed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reason for Using These Methods\n",
    "\n",
    "Each preprocessing step is chosen based on common practices in NLP to improve the quality of input data for machine learning models:\n",
    "\n",
    "- **Lowercasing**: Helps in treating words like \"Hello\", \"HELLO\", and \"hello\" equally.\n",
    "- **Removing numbers and special characters**: Often, numbers and special characters do not carry useful information for sentiment analysis and can introduce noise into the data.\n",
    "- **Stopword removal**: Stopwords are common words that usually do not carry significant meaning and are removed to reduce the dataset size and improve processing time.\n",
    "- **Stemming and Lemmatization**: These processes reduce words to their root form, helping the model to understand that different forms of a word carry the same meaning.\n",
    "\n",
    "When implementing these preprocessing steps, it's important to remember that the effectiveness of each method can vary depending on the dataset and the specific task at hand. It's always a good idea to experiment with different preprocessing techniques and evaluate their impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the modeling quality using different text vectorization methods along with Logistic Regression and K-Nearest Neighbors (KNN) algorithms, we'll proceed through several steps:\n",
    "\n",
    "1. **High-level text preprocessing** of the dataset using the third method described previously.\n",
    "2. **Vectorization**: We'll use two vectorization methods: Bag of Words (BoW) and Word2Vec (although \"vec2Word\" was mentioned, I'll assume it's a reference to Word2Vec, a common word embedding technique).\n",
    "3. **Model Training and Hyperparameter Tuning**: For Logistic Regression and K-Nearest Neighbors (KNN), we'll adjust hyperparameters to find the best performing model.\n",
    "4. **Evaluation and Reporting**: We'll evaluate the models using appropriate metrics (like accuracy, precision, recall, and F1-score) and report the best settings.\n",
    "\n",
    "### Preparing the Environment\n",
    "\n",
    "First, ensure you have all necessary libraries installed and imported. We'll need `sklearn`, `gensim` for Word2Vec, `nltk` for text preprocessing, and `pandas` for data manipulation.\n",
    "\n",
    "### High-level Text Preprocessing\n",
    "\n",
    "We'll preprocess the dataset using the high-level text preprocessing method as described previously. This step is assumed to be completed, and the preprocessed dataset is ready for vectorization.\n",
    "\n",
    "### Vectorization Methods Implementation\n",
    "\n",
    "#### Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize_bow(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "\n",
    "For Word2Vec, we first need to train a model on our corpus or use a pre-trained model. Given the constraint of not using external data, we'll train a simple Word2Vec model on the preprocessed comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def train_word2vec(sentences):\n",
    "    # Train a Word2Vec model\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "def vectorize_word2vec(model, sentences):\n",
    "    # Vectorize sentences using the average of word vectors\n",
    "    vectorized = []\n",
    "    for sentence in sentences:\n",
    "        vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "        if vectors:\n",
    "            vectorized.append(np.mean(vectors, axis=0))\n",
    "        else:\n",
    "            vectorized.append(np.zeros(model.vector_size))\n",
    "    return np.array(vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Hyperparameter Tuning\n",
    "\n",
    "We'll split the dataset into training and test sets, then train and tune both Logistic Regression and KNN models for each vectorization method.\n",
    "\n",
    "#### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `data_high_level_preprocessed` is the dataset after high-level preprocessing\n",
    "X = data_high_level_preprocessed['comment']\n",
    "y = data_high_level_preprocessed['sentiment']\n",
    "\n",
    "# For BoW\n",
    "X_bow, vectorizer_bow = vectorize_bow(X)\n",
    "# Splitting for BoW\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# For Word2Vec\n",
    "# Tokenize the sentences for Word2Vec\n",
    "sentences = [nltk.word_tokenize(comment) for comment in X]\n",
    "w2v_model = train_word2vec(sentences)\n",
    "X_w2v = vectorize_word2vec(w2v_model, sentences)\n",
    "# Splitting for Word2Vec\n",
    "X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Hyperparameter Tuning\n",
    "\n",
    "Due to the complexity and the computational cost of hyperparameter tuning, I'll outline the approach for one model and vectorization method combination. You can replicate this approach for the other combinations.\n",
    "\n",
    "##### Logistic Regression with BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize the classifier\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit on the training data\n",
    "grid_search.fit(X_train_bow, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best score: {best_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
